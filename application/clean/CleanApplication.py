from io import StringIO
import json, sys, os, re
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
import application.common as cmn
from query import PipeTaskStatus, QueryPipeTaskClean, QueryPipeTaskSearch
from earthling.connector.s3_module import read_file_from_s3
import numpy as np
from collections import Counter
from collections import defaultdict
from application.clean.config_adapter import (
    DOMAIN_STOPWORDS, MORPHEME_PATTERNS, REPEAT_PATTERNS,
    SEMANTIC_CLUSTERS, NGRAM_STOPWORDS, MEANINGLESS_AFFIXES, 
    CONTEXT_STOPWORDS, POS_MIN_LENGTH, MULTI_WORD_EXPRESSIONS, STOPWORDS, 
    load_domain_stopwords, refresh_all_configs
)

from konlpy.tag import Okt
# print("âœ… KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì„±ê³µ")
MORPHEME_ANALYZER_AVAILABLE = True

class OktAnalyzer:
    def __init__(self):
        self.tagger = Okt()
    
    def pos(self, text):
        # Okt í’ˆì‚¬ íƒœê¹… ê²°ê³¼ ë°˜í™˜
        return self.tagger.pos(text)

morpheme_analyzer = OktAnalyzer()
    
class CleanApplication:
    def __init__(self):
        # ë™ì  ë¶ˆìš©ì–´
        self.dynamic_stopwords = set()
        self.semantic_stopwords = set()
        self.frequency_stopwords = set()
        self.context_stopwords = set()
        
        # í†µê³„ ì •ë³´
        self.word_stats = {}
        self.pos_stats = {}        
        
    def get_all_active_stopwords(self):
        """í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“  ë¶ˆìš©ì–´ ë°˜í™˜"""
        base_stopwords = STOPWORDS
        all_active = base_stopwords.copy()
        all_active.update(self.dynamic_stopwords)
        all_active.update(self.frequency_stopwords)
        all_active.update(self.context_stopwords)
        all_active.update(self.semantic_stopwords)
        return all_active  
    
    def detect_domain(self, text_list):
        """í…ìŠ¤íŠ¸ ë„ë©”ì¸ ìë™ ê°ì§€"""
        domain_scores = {domain: 0 for domain in DOMAIN_STOPWORDS.keys()}
        
        for text in text_list[:100]:  # ìƒ˜í”Œë§
            text_lower = text.lower()
            for domain, keywords in DOMAIN_STOPWORDS.items():
                for keyword in keywords:
                    if keyword in text_lower:
                        domain_scores[domain] += 1
        
        return max(domain_scores.items(), key=lambda x: x[1])[0] if max(domain_scores.values()) > 0 else 'general'

    def apply_semantic_clustering(self, word_list):
        """ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ë¶ˆìš©ì–´ ì¶”ì¶œ"""
        # print("ğŸ§  ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
        
        # ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¨ì–´ ë¹ˆë„ í™•ì¸
        for cluster_name, cluster_words in SEMANTIC_CLUSTERS.items():
            cluster_count = sum(1 for word in word_list if word in cluster_words)
            cluster_ratio = cluster_count / len(word_list) if word_list else 0
            
            # í´ëŸ¬ìŠ¤í„° ë‹¨ì–´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë¶ˆìš©ì–´ë¡œ ì¶”ê°€
            if cluster_ratio > 0.02:  # ì „ì²´ì˜ 2% ì´ìƒ
                self.semantic_stopwords.update(cluster_words)
    
    def apply_ngram_analysis(self, word_list):
        """N-gram íŒ¨í„´ ê¸°ë°˜ ë¶ˆìš©ì–´ ë¶„ì„"""
        # print("ğŸ“Š N-gram íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        # Bi-gram ë¶„ì„
        bigrams = [(word_list[i], word_list[i+1]) for i in range(len(word_list)-1)]
        bigram_counts = {}
        for bigram in bigrams:
            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1
        
        # NGRAM_STOPWORDSì— ì •ì˜ëœ íŒ¨í„´ê³¼ ë§¤ì¹­ë˜ëŠ” ë‹¨ì–´ë“¤ ì œê±°
        for bigram_tuple in NGRAM_STOPWORDS.get('bigram', []):
            if bigram_tuple in bigram_counts and bigram_counts[bigram_tuple] > 3:
                self.semantic_stopwords.update(bigram_tuple)
                # print(f"   ğŸ¯ ê³ ë¹ˆë„ Bi-gram: {' '.join(bigram_tuple)}")
        
        # Tri-gram ë¶„ì„
        trigrams = [(word_list[i], word_list[i+1], word_list[i+2]) for i in range(len(word_list)-2)]
        trigram_counts = {}
        for trigram in trigrams:
            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1
        
        for trigram_tuple in NGRAM_STOPWORDS.get('trigram', []):
            if trigram_tuple in trigram_counts and trigram_counts[trigram_tuple] > 2:
                self.semantic_stopwords.update(trigram_tuple)
                # print(f"   ğŸ¯ ê³ ë¹ˆë„ Tri-gram: {' '.join(trigram_tuple)}")
    
    def extract_contextual_stopwords(self, word_list):
        """ë¬¸ë§¥ ê¸°ë°˜ ë™ì  ë¶ˆìš©ì–´ ì¶”ì¶œ"""
        # ë™ì‚¬ í™œìš©í˜• íŒ¨í„´ ê°ì§€
        verb_patterns = defaultdict(set)
        for word in word_list:
            for base_verb, variations in CONTEXT_STOPWORDS.items():
                if any(word.startswith(var[:2]) for var in variations):
                    verb_patterns[base_verb].add(word)
        
        # ë„ˆë¬´ ë§ì€ í™œìš©í˜•ì´ ìˆëŠ” ë™ì‚¬ëŠ” ë¶ˆìš©ì–´ë¡œ ì¶”ê°€
        for base_verb, variations in verb_patterns.items():
            if len(variations) > 5:                
                self.context_stopwords.update(variations)        

    def build_advanced_stopwords(self, word_list, min_freq=2, max_freq_ratio=0.6):
        """ëª¨ë“  íŒ¨í„´ì„ í™œìš©í•œ ê³ ê¸‰ ë¶ˆìš©ì–´ ìƒì„±"""
        from collections import Counter
        
        word_counts = Counter(word_list)
        total_words = len(word_list)
        
        print(f"ğŸš€ ê³ ê¸‰ ë¶ˆìš©ì–´ ë¶„ì„ ì‹œì‘: ì „ì²´ {total_words}ê°œ ë‹¨ì–´, ê³ ìœ  {len(word_counts)}ê°œ")
        
        # 1. ê¸°ë³¸ ë¹ˆë„ ê¸°ë°˜ ë¶ˆìš©ì–´
        for word, count in word_counts.items():
            freq_ratio = count / total_words
            if count < min_freq:
                self.frequency_stopwords.add(word)
            elif freq_ratio > max_freq_ratio:
                self.frequency_stopwords.add(word)
        
        # 2. ë¬¸ë§¥ ê¸°ë°˜ ë¶ˆìš©ì–´ (CONTEXT_STOPWORDS í™œìš©)
        self.extract_contextual_stopwords(word_list)
        
        # 3. ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„ (SEMANTIC_CLUSTERS í™œìš©)
        self.apply_semantic_clustering(word_list)
        
        # 4. N-gram íŒ¨í„´ ë¶„ì„ (NGRAM_STOPWORDS í™œìš©)
        self.apply_ngram_analysis(word_list)                  
    
    def apply_compound_aware_filtering(self, word, pos):
        """í•©ì„±ì–´ì™€ ë³µí•© ëª…ì‚¬ë¥¼ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§"""
        # ê¸°ë³¸ ê¸¸ì´ ì²´í¬
        min_length = POS_MIN_LENGTH.get(pos, 2)
        if len(word) < min_length:
            return False
        
        # ê¸°ë³¸ ë¶ˆìš©ì–´ ì²´í¬ (ëª¨ë“  í™œì„± ë¶ˆìš©ì–´ í¬í•¨)
        active_stopwords = self.get_all_active_stopwords()
        if word in active_stopwords:
            return False
        
        # ë³µí•© ëª…ì‚¬ì¸ ê²½ìš° ë³´í˜¸ (ë„ì–´ì“°ê¸° í¬í•¨)
        if ' ' in word:
            # ë³µí•© ëª…ì‚¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë³´ì¡´
            return True
        
        # ë¯¸ë¦¬ ì •ì˜ëœ ë³µí•© ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš° ë³´í˜¸
        for compound in MULTI_WORD_EXPRESSIONS:
            if word == compound.replace(' ', ''):  # ê³µë°± ì œê±° í›„ ë¹„êµ
                return True    
        
        for pattern in REPEAT_PATTERNS:
            if re.search(pattern, word):
                return False
        
        # í˜•íƒœì†Œ ê¸°ë°˜ íŒ¨í„´ ì²´í¬ (MORPHEME_PATTERNS)
        for pattern_type, patterns in MORPHEME_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, word):
                    return False
        
        # 6. ì˜ë¯¸ì—†ëŠ” ì ‘ë‘ì–´/ì ‘ë¯¸ì–´ ì²´í¬ (MEANINGLESS_AFFIXES)
        for prefix in MEANINGLESS_AFFIXES.get('prefix', []):
            if word.startswith(prefix) and len(word) > len(prefix):
                return False
        for suffix in MEANINGLESS_AFFIXES.get('suffix', []):
            if word.endswith(suffix) and len(word) > len(suffix):
                return False
        for infix in MEANINGLESS_AFFIXES.get('infix', []):
            if infix in word and len(word) > len(infix):
                return False
        
        # í•œê¸€ë§Œ í—ˆìš©
        if not re.match(r'^[ê°€-í£]+$', word):
            return False
        
        # ë¬¸ì ë‹¤ì–‘ì„± ì²´í¬ (í•©ì„±ì–´ê°€ ì•„ë‹Œ ê²½ìš° ë” ì—„ê²©í•˜ê²Œ)
        if len(word) > 2:
            unique_chars = len(set(word))
            total_chars = len(word)
            if unique_chars / total_chars < 0.4:  # í•©ì„±ì–´ê°€ ì•„ë‹ˆë©´ ë” ì—„ê²©
                return False
        
        return True

    def preprocess_compound_nouns(self, text):
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë³µí•© ëª…ì‚¬ë¥¼ ì‹ë³„í•˜ê³  í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì—°ê²°
        'ì½”ì¹´ ì½œë¼' â†’ 'ì½”ì¹´_ì½œë¼'ë¡œ ë³€í™˜í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ì—ì„œ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì²˜ë¦¬
        """
        processed_text = text
        found_compounds = []
        
        # 1. ë¯¸ë¦¬ ì •ì˜ëœ ë³µí•© ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì ìš© (ë‹¨ìˆœ ë¬¸ìì—´ ë§¤ì¹­)
        for compound in MULTI_WORD_EXPRESSIONS:
            if compound in text:
                compound_token = compound.replace(' ', '_')
                processed_text = processed_text.replace(compound, compound_token)
                found_compounds.append(compound)
        
        # 2. íŒ¨í„´ ê¸°ë°˜ ë³µí•© ëª…ì‚¬ íƒì§€ ë° ì—°ê²° (ê¸°ë³¸ íŒ¨í„´ë§Œ)
        import re
        basic_pattern = r'([ê°€-í£]{2,4})\\s([ê°€-í£]{2,4})'
        matches = re.finditer(basic_pattern, processed_text)
        for match in matches:
            original = match.group()
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë³µí•© ëª…ì‚¬ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
            if not any(compound in original for compound in found_compounds):
                compound_token = original.replace(' ', '_')
                processed_text = processed_text.replace(original, compound_token, 1)
                found_compounds.append(original)
        
        return processed_text
    
    def postprocess_compound_tokens(self, tokens):
        """
        í˜•íƒœì†Œ ë¶„ì„ í›„ ë³µí•© ëª…ì‚¬ í† í°ì„ ì›ë˜ í˜•íƒœë¡œ ë³µì›
        'ì½”ì¹´_ì½œë¼' â†’ 'ì½”ì¹´ ì½œë¼'
        """
        processed_tokens = []
        for token, pos in tokens:
            if '_' in token:
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ë‹¤ì‹œ ë„ì–´ì“°ê¸°ë¡œ ë³€í™˜
                restored_token = token.replace('_', ' ')
                processed_tokens.append([restored_token, pos])
            else:
                processed_tokens.append([token, pos])
        return processed_tokens

    def execute(self, task):
        # print("ğŸ”„ ì„¤ì • ìƒˆë¡œê³ ì¹¨ ì‹œë„...")
        refresh_all_configs()
        # print("âœ… ì„¤ì • ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ.")

        # print("\n--- í˜„ì¬ ì ìš©ëœ ë™ì  ì„¤ì • ---")
        # print(f"  - ë°˜ë³µ íŒ¨í„´ (REPEAT_PATTERNS): {len(REPEAT_PATTERNS)}ê°œ")
        # print(f"  - N-gram ë¶ˆìš©ì–´ (NGRAM_STOPWORDS): bigrams={len(NGRAM_STOPWORDS.get('bigram', []))}, trigrams={len(NGRAM_STOPWORDS.get('trigram', []))}")
        # print(f"  - ë¬´ì˜ë¯¸ ì ‘ì‚¬ (MEANINGLESS_AFFIXES): prefixes={len(MEANINGLESS_AFFIXES.get('prefix', []))}, suffixes={len(MEANINGLESS_AFFIXES.get('suffix', []))}, infixes={len(MEANINGLESS_AFFIXES.get('infix', []))}")
        # print(f"  - ë¬¸ë§¥ ë¶ˆìš©ì–´ (CONTEXT_STOPWORDS): {len(CONTEXT_STOPWORDS)}ê°œ")
        # print(f"  - í’ˆì‚¬ë³„ ìµœì†Œ ê¸¸ì´ (POS_MIN_LENGTH): {POS_MIN_LENGTH}")
        # print("--------------------------\n")
          
        search_task_id = task["search_task_id"]
        query = QueryPipeTaskSearch()
        searched = query.get_task_by_id(search_task_id)
        origin_url = searched.s3_url

        query = QueryPipeTaskClean()
        task_no = task["id"]
        query.update_search_status_start_date_to_now(task_no)
        resource_origin = read_file_from_s3(origin_url)

        target_list = []
        resource_list = resource_origin.split("\n")
        for resource in resource_list:
            line = resource.split("\t")            
            if len(line) < 2:
                continue
            title, text = line[0], line[2]
            target_text = f"{title} {text}"
            target_list.append(target_text)
        
        # ë„ë©”ì¸ ìë™ ê°ì§€ ë° ë„ë©”ì¸ë³„ ë¶ˆìš©ì–´ ì¶”ê°€ (ì™¸ë¶€ ì„¤ì • íŒŒì¼ ì‚¬ìš©)
        detected_domain = self.detect_domain(target_list)
        domain_stopwords = load_domain_stopwords(detected_domain)
        # if domain_stopwords:
        #     print(f"ğŸ·ï¸ ê°ì§€ëœ ë„ë©”ì¸: {detected_domain} (ë¶ˆìš©ì–´ {len(domain_stopwords)}ê°œ ì¶”ê°€)")
        
        target_pos = {"Noun", "ProperNoun", "Verb", "Adjective"}  # Okt í’ˆì‚¬ íƒœê·¸
        
        # 1ì°¨: ë³µí•© ëª…ì‚¬ ì „ì²˜ë¦¬ ë° ì „ì²´ ë‹¨ì–´ ìˆ˜ì§‘ (ë™ì  ë¶ˆìš©ì–´ ìƒì„±ìš©)
        all_words = []
        temp_results = []
        
        for text in target_list:            # ë³µí•© ëª…ì‚¬ ì „ì²˜ë¦¬: 'ì½”ì¹´ ì½œë¼' â†’ 'ì½”ì¹´_ì½œë¼'
            preprocessed_text = self.preprocess_compound_nouns(text)
            pos_list = morpheme_analyzer.pos(preprocessed_text)
            # ë³µí•© ëª…ì‚¬ í† í° í›„ì²˜ë¦¬: 'ì½”ì¹´_ì½œë¼' â†’ 'ì½”ì¹´ ì½œë¼'
            pos_list = self.postprocess_compound_tokens(pos_list)            
            words_in_text = [word for word, pos in pos_list if pos in target_pos]
            all_words.extend(words_in_text)
            temp_results.append(pos_list)
        
        # ê°„ë‹¨í•œ ë™ì  ë¶ˆìš©ì–´ ìƒì„± â†’ ê³ ê¸‰ ë¶ˆìš©ì–´ ìƒì„±ìœ¼ë¡œ ë³€ê²½
        self.build_advanced_stopwords(all_words)        
        
        # 2ì°¨: ìµœì¢… í•„í„°ë§ (ëª¨ë“  ê³ ê¸‰ íŒ¨í„´ ì ìš©)
        pos_filited_list = []
        total_words = 0
        filtered_words = 0
        
        for pos_list in temp_results:
            filtered = []
            for word, pos in pos_list:
                total_words += 1
                if pos in target_pos and self.apply_compound_aware_filtering(word, pos):
                    filtered.append([word, pos])
                    filtered_words += 1
            pos_filited_list.append(filtered)
        
        # print(f"ğŸ” ê³ ê¸‰ í•„í„°ë§ ê²°ê³¼: {total_words}ê°œ â†’ {filtered_words}ê°œ (ì œê±°ìœ¨: {(1-filtered_words/total_words)*100:.1f}%)")
        morph_data = [[list(pair) for pair in sublist] for sublist in pos_filited_list]
        
        converted = cmn.convert_morph_to_json(morph_data)
        # for cvt in converted:
        #     print(cvt)

        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 3ê°œ ë¬¸ì„œì˜ ë‹¨ì–´ë“¤)
        # for i, doc in enumerate(morph_data[:3]):
        #   if doc:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
        #       words = [pair[0] for pair in doc]
              # print(f"   ë¬¸ì„œ {i+1}: {', '.join(words[:10])}{'...' if len(words) > 10 else ''}")
        
        # ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤ì œ ì €ì¥
        filename = cmn.get_save_filename(cmn.AppType.CLEAN)
        buffer = StringIO()
        json.dump(converted, buffer, ensure_ascii=False, indent=2)
        cmn.save_to_s3_and_update_with_buffer(query, task_no, filename, buffer)
        query.update_state_to_completed(task_no)

        query.update_state_to_pending_about_analysis_task(PipeTaskStatus.FREQUENCY, task_no)
        query.update_state_to_pending_about_analysis_task(PipeTaskStatus.TFIDF, task_no)
        query.update_state_to_pending_about_analysis_task(PipeTaskStatus.CONCOR, task_no)

if __name__ == "__main__":
    app = CleanApplication()
    app.execute({"id": 26, "search_task_id": 64})